{
	"name": "01-data-ingestion",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f01ba318-0a5b-4215-a3c4-83789e08fa85"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/98fb3285-f648-4b06-80a4-01c259978e8f/resourceGroups/flights-mlops-proof-of-concept/providers/Microsoft.Synapse/workspaces/synw-flights-bronze-layer/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://synw-flights-bronze-layer.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 3600
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession, DataFrame\r\n",
					"from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\r\n",
					"from datetime import datetime, timedelta\r\n",
					"import uuid\r\n",
					"import random\r\n",
					"from functools import reduce"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"## Pass parameter from Synapse dataflow\r\n",
					"'''\r\n",
					"# Date range from 2023-07-01 to 2024-07-01\r\n",
					"start_date = datetime(2023, 7, 1)\r\n",
					"end_date = datetime(2023, 7, 31)\r\n",
					"date_range = [start_date + timedelta(days=i) for i in range((end_date - start_date).days)]\r\n",
					"'''\r\n",
					"\r\n",
					"start_date_param = \"\"\r\n",
					"end_date_param = \"\"\r\n",
					"dummy_rows_param = \"\""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": []
				},
				"source": [
					"################ Params ################\r\n",
					"\r\n",
					"start_date = datetime.strptime(start_date_param, \"%Y-%m-%d\")\r\n",
					"end_date = datetime.strptime(end_date_param, \"%Y-%m-%d\")\r\n",
					"\r\n",
					"\r\n",
					"# Providers\r\n",
					"providers = [\r\n",
					"    \"TRAVELFUSION/americanAirlines\", \"AMADEUS/voyzant\", \"AMADEUS/picasso\",\r\n",
					"    \"AMADEUS/aerticket\", \"AMADEUS/amadeus\", \"TRAVELFUSION/lufthansa\", \"TRAVELFUSION/travelfusion\"\r\n",
					"]\r\n",
					"\r\n",
					"# Routes\r\n",
					"routes = [\r\n",
					"    'EWR-CAI', 'FLL-LIS', 'JFK-ATH', 'JFK-CAI', 'LAX-ATH', 'LGA-CAI', 'LGW-IST', 'LGW-NRT', 'LHR-AMM',\r\n",
					"    'LHR-DEL', 'LHR-HND', 'LHR-IST', 'LHR-LIM', 'LHR-NRT', 'LHR-SAW', 'MIA-LIS', 'STN-AMM', 'STN-SAW',\r\n",
					"    'YTZ-ATH', 'YTZ-LIS', 'YYZ-ATH', 'YYZ-JNB', 'YYZ-LIS', 'JFK-JNB', 'YYZ-UIO', 'LAX-JNB', 'LHR-JNB',\r\n",
					"    'LHR-SJO', 'SFO-LIS', 'YUL-LIS', 'LAX-LIM', 'YYZ-LIM', 'LAX-LIS', 'MIA-LIM', 'LAX-IST', 'LAX-UIO'\r\n",
					"]\r\n",
					"\r\n",
					"# Currencies\r\n",
					"currencies = [\r\n",
					"    \"EUR\", \"COP\", \"MXN\", \"USD\", \"CAD\", \"GBP\"\r\n",
					"]\r\n",
					"\r\n",
					"baggage = [\r\n",
					"    \"0PC\", \"1PC\", \"2PC\", \"3PC\"\r\n",
					"]\r\n",
					"\r\n",
					"\r\n",
					"################ Functions ################\r\n",
					"\r\n",
					"# Function to generate rows in PySpark DataFrame format\r\n",
					"def generate_rows_for_date(date, num_rows):\r\n",
					"    rows = []\r\n",
					"    for _ in range(num_rows):\r\n",
					"        total_person = random.randint(1, 5)\r\n",
					"        total_flight_cost = random.uniform(21.23, 100000.4)\r\n",
					"        total_flight_cost_per_passenger = total_flight_cost / total_person\r\n",
					"        rows.append((\r\n",
					"            str(uuid.uuid4()),  # id\r\n",
					"            f\"L1{{{random.choice(routes)}}}_L2{random.randint(1000, 9999)}{random.choice(baggage)}\",  # flight_code\r\n",
					"            f\"flightHistory/{date.strftime('%Y-%m-%d')}/{uuid.uuid4().hex}.json.gz\",  # file_path\r\n",
					"            random.choice(providers),  # source\r\n",
					"            date.strftime(\"%Y-%m-%d\"),  # search_date\r\n",
					"            (date + timedelta(days=10, hours=10, minutes=25)).strftime(\"%Y-%m-%d %H:%M:%S\"),  # outbound_departure_datetime\r\n",
					"            (date + timedelta(days=15, hours=10, minutes=25)).strftime(\"%Y-%m-%d %H:%M:%S\"),  # inbound_departure_datetime\r\n",
					"            random.choice(routes),  # route\r\n",
					"            \"BUE\",  # city_pair\r\n",
					"            \"AR\",  # country\r\n",
					"            \"CAB_ECONOMY\",  # cabin_class\r\n",
					"            \"AEP\",  # airport_code\r\n",
					"            \"publishedFare\",  # fare_type\r\n",
					"            total_person,  # passengers\r\n",
					"            random.randint(0, 1),  # infants\r\n",
					"            random.randint(0, 2),  # children\r\n",
					"            total_person,  # adults\r\n",
					"            random.choice(currencies),  # currency\r\n",
					"            total_flight_cost,  # total_cost\r\n",
					"            total_flight_cost_per_passenger,  # cost_per_adult\r\n",
					"            random.choice(baggage),  # baggage_allowance\r\n",
					"            date.strftime(\"%Y-%m-%d %H:%M:%S\")  # last_update\r\n",
					"        ))\r\n",
					"    return rows\r\n",
					"\r\n",
					"#### Create the dataframe\r\n",
					"def create_dataframe(rows_list):\r\n",
					"    # Create RDD from all_rows\r\n",
					"    rdd = spark.sparkContext.parallelize(rows_list)\r\n",
					"\r\n",
					"    # Create DataFrame schema\r\n",
					"    schema = StructType([\r\n",
					"        StructField(\"id\", StringType(), True),\r\n",
					"        StructField(\"flight_code\", StringType(), True),\r\n",
					"        StructField(\"file_path\", StringType(), True),\r\n",
					"        StructField(\"source\", StringType(), True),\r\n",
					"        StructField(\"search_date\", StringType(), True),\r\n",
					"        StructField(\"outbound_departure_datetime\", StringType(), True),\r\n",
					"        StructField(\"inbound_departure_datetime\", StringType(), True),\r\n",
					"        StructField(\"route\", StringType(), True),\r\n",
					"        StructField(\"city_pair\", StringType(), True),\r\n",
					"        StructField(\"country\", StringType(), True),\r\n",
					"        StructField(\"cabin_class\", StringType(), True),\r\n",
					"        StructField(\"airport_code\", StringType(), True),\r\n",
					"        StructField(\"fare_type\", StringType(), True),\r\n",
					"        StructField(\"passengers\", IntegerType(), True),\r\n",
					"        StructField(\"infants\", IntegerType(), True),\r\n",
					"        StructField(\"children\", IntegerType(), True),\r\n",
					"        StructField(\"adults\", IntegerType(), True),\r\n",
					"        StructField(\"currency\", StringType(), True),\r\n",
					"        StructField(\"total_cost\", DoubleType(), True),\r\n",
					"        StructField(\"cost_per_adult\", DoubleType(), True),\r\n",
					"        StructField(\"baggage_allowance\", StringType(), True),\r\n",
					"        StructField(\"last_update\", StringType(), True)\r\n",
					"    ])\r\n",
					"\r\n",
					"    # Create DataFrame from RDD applying the schema\r\n",
					"    df = spark.createDataFrame(rdd, schema=schema)\r\n",
					"    print(\"###--- Dataframe generation executed! ---###\")\r\n",
					"    return df\r\n",
					"\r\n",
					"### Write in DataLakeGen v2\r\n",
					"def write_datalake(dataframe):\r\n",
					"    account_name = \"stflightshistorical001\"\r\n",
					"    container_name = \"azfs-flights-historical\"\r\n",
					"    relative_path = \"flights-historical\" \r\n",
					"\r\n",
					"    # Repartition the DataFrame\r\n",
					"    dataframe = dataframe.repartition(\"search_date\", \"route\")\r\n",
					"\r\n",
					"    # Write DataFrame to Azure Data Lake Gen2 in Parquet format partitioned by search_date and route\r\n",
					"    dataframe.write \\\r\n",
					"        .partitionBy(\"search_date\", \"route\") \\\r\n",
					"        .option(\"compression\", \"snappy\") \\\r\n",
					"        .mode(\"append\") \\\r\n",
					"        .parquet(f\"abfss://{container_name}@{account_name}.dfs.core.windows.net/{relative_path}/\")\r\n",
					"    \r\n",
					"    print(f\"###--- Write datalake executed for this search_date---###\")\r\n",
					"\r\n",
					"\r\n",
					"# Function to generate and write data in batches\r\n",
					"def generate_and_write_data(start_date, end_date, batch_row_threshold, dummy_rows_param=\"true\"):\r\n",
					"    current_date = start_date\r\n",
					"    \r\n",
					"    while current_date <= end_date:\r\n",
					"        if dummy_rows_param == 'true':\r\n",
					"            total_rows_for_day = random.randint(100_000, 1_000_000)\r\n",
					"        if dummy_rows_param == 'false':\r\n",
					"            total_rows_for_day = random.randint(1_000, 10_500)\r\n",
					"        rows_accumulated = 0\r\n",
					"\r\n",
					"        while rows_accumulated < total_rows_for_day:\r\n",
					"            rows_to_generate = min(batch_row_threshold, total_rows_for_day - rows_accumulated)\r\n",
					"            rows = generate_rows_for_date(current_date, rows_to_generate)\r\n",
					"            temp_df = create_dataframe(rows)\r\n",
					"            write_datalake(temp_df)\r\n",
					"            rows_accumulated += rows_to_generate\r\n",
					"\r\n",
					"            print(f\"Date: {current_date} | Rows written: {rows_accumulated}/{total_rows_for_day}\")\r\n",
					"        \r\n",
					"        current_date += timedelta(days=1)\r\n",
					"\r\n",
					"# Usage example\r\n",
					"batch_row_threshold = 50_000  # Write data in batches of 1,000,000 rows\r\n",
					"generate_and_write_data(start_date, end_date, batch_row_threshold)"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.close"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"'''\r\n",
					"Compression ratio\r\n",
					"\r\n",
					"\r\n",
					"When compressing large datasets partitioned in a data lake using PySpark, the choice of compression codec can significantly affect both the performance and the size of the saved files. Below are some commonly used compression codecs, with a brief overview of their performance and suitability for large datasets:\r\n",
					"\r\n",
					"    Snappy:\r\n",
					"        Pros: Fast compression and decompression, reasonable compression ratio.\r\n",
					"        Cons: Slightly larger file sizes compared to other codecs like gzip or zstd.\r\n",
					"        Best Use: When speed is a priority over the compression ratio.\r\n",
					"    Gzip:\r\n",
					"        Pros: Good compression ratio.\r\n",
					"        Cons: Slower compression and decompression compared to snappy.\r\n",
					"        Best Use: When the compression ratio is more important than speed.\r\n",
					"    LZO:\r\n",
					"        Pros: Fast compression and decompression, reasonable compression ratio.\r\n",
					"        Cons: Not as widely supported as snappy or gzip.\r\n",
					"        Best Use: Similar to snappy, for scenarios where speed is important.\r\n",
					"    Brotli:\r\n",
					"        Pros: Very good compression ratio.\r\n",
					"        Cons: Slower than gzip, but faster than zstd in many cases.\r\n",
					"        Best Use: When you need a good balance between compression ratio and speed.\r\n",
					"    LZ4:\r\n",
					"        Pros: Extremely fast compression and decompression.\r\n",
					"        Cons: Lower compression ratio compared to gzip or zstd.\r\n",
					"        Best Use: When decompression speed is critical (e.g., real-time applications).\r\n",
					"    Zstandard (zstd):\r\n",
					"        Pros: Excellent compression ratio, good speed.\r\n",
					"        Cons: Slightly slower than snappy but generally faster than gzip.\r\n",
					"        Best Use: When you need a high compression ratio with reasonable speed.\r\n",
					"\r\n",
					"Recommended Choice:\r\n",
					"\r\n",
					"    For general purposes: Snappy is often recommended due to its balance of speed and compression ratio. It's particularly suitable for big data processing where quick read/write operations are crucial.\r\n",
					"    For better compression ratio: Zstandard (zstd) is an excellent choice as it offers a good balance of speed and compression efficiency.\r\n",
					"    For maximum compression ratio: Gzip or Brotli can be considered, but they are slower compared to Snappy and Zstandard.\r\n",
					"    \r\n",
					"'''"
				]
			}
		]
	}
}