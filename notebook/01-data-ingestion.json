{
	"name": "01-data-ingestion",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "ad4c0bb2-3c77-4fc1-872b-c6ca680d8ce9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/98fb3285-f648-4b06-80a4-01c259978e8f/resourceGroups/flights-mlops-proof-of-concept/providers/Microsoft.Synapse/workspaces/synw-flights-bronze-layer/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://synw-flights-bronze-layer.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import StringType, IntegerType, DoubleType, StructType, StructField\r\n",
					"from datetime import datetime, timedelta\r\n",
					"import uuid\r\n",
					"import random\r\n",
					"\r\n",
					"# Date range from 2023-07-01 to 2024-07-01\r\n",
					"start_date = datetime(2023, 7, 1)\r\n",
					"end_date = datetime(2024, 7, 1)\r\n",
					"date_range = [start_date + timedelta(days=i) for i in range((end_date - start_date).days)]"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Params\r\n",
					"\r\n",
					"# Providers\r\n",
					"providers = [\r\n",
					"    \"TRAVELFUSION/americanAirlines\", \"AMADEUS/voyzant\", \"AMADEUS/picasso\",\r\n",
					"    \"AMADEUS/aerticket\", \"AMADEUS/amadeus\", \"TRAVELFUSION/lufthansa\", \"TRAVELFUSION/travelfusion\"\r\n",
					"]\r\n",
					"\r\n",
					"# Routes\r\n",
					"routes = [\r\n",
					"    'EWR-CAI', 'FLL-LIS', 'JFK-ATH', 'JFK-CAI', 'LAX-ATH', 'LGA-CAI', 'LGW-IST', 'LGW-NRT', 'LHR-AMM',\r\n",
					"    'LHR-DEL', 'LHR-HND', 'LHR-IST', 'LHR-LIM', 'LHR-NRT', 'LHR-SAW', 'MIA-LIS', 'STN-AMM', 'STN-SAW',\r\n",
					"    'YTZ-ATH', 'YTZ-LIS', 'YYZ-ATH', 'YYZ-JNB', 'YYZ-LIS', 'JFK-JNB', 'YYZ-UIO', 'LAX-JNB', 'LHR-JNB',\r\n",
					"    'LHR-SJO', 'SFO-LIS', 'YUL-LIS', 'LAX-LIM', 'YYZ-LIM', 'LAX-LIS', 'MIA-LIM', 'LAX-IST', 'LAX-UIO'\r\n",
					"]\r\n",
					"\r\n",
					"# Currencies\r\n",
					"currencies = [\r\n",
					"    \"EUR\", \"COP\", \"MXN\", \"USD\", \"CAD\", \"GBP\"\r\n",
					"]\r\n",
					"\r\n",
					"baggage = [\r\n",
					"    \"0PC\", \"1PC\", \"2PC\", \"3PC\"\r\n",
					"]"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Function to generate rows in PySpark DataFrame format\r\n",
					"def generate_rows_for_date(date):\r\n",
					"    num_rows = random.randint(100_000, 1_500_000)  # Generate between 5,000,000 to 150,000,000 rows per day randomly.\r\n",
					"    print(f\"The total of rows generated is {num_rows}\")\r\n",
					"\r\n",
					"    rows = []\r\n",
					"    for _ in range(num_rows):\r\n",
					"        total_person = random.randint(1, 5)\r\n",
					"        total_flight_cost = random.uniform(21.23, 100000.4)\r\n",
					"        total_flight_cost_per_passenger = total_flight_cost / total_person\r\n",
					"        rows.append((\r\n",
					"            str(uuid.uuid4()),                                               # id\r\n",
					"            f\"L1{{{random.choice(routes)}}}_L2{random.randint(1000, 9999)}{random.choice(baggage)}\",  # flight_code\r\n",
					"            f\"flightHistory/{date.strftime('%Y-%m-%d')}/{uuid.uuid4().hex}.json.gz\",  # file_path\r\n",
					"            random.choice(providers),                                         # source\r\n",
					"            date.strftime(\"%Y-%m-%d\"),                                         # search_date\r\n",
					"            (date + timedelta(days=10, hours=10, minutes=25)).strftime(\"%Y-%m-%d %H:%M:%S\"),  # outbound_departure_datetime\r\n",
					"            (date + timedelta(days=15, hours=10, minutes=25)).strftime(\"%Y-%m-%d %H:%M:%S\"),  # inbound_departure_datetime\r\n",
					"            random.choice(routes),                                            # route\r\n",
					"            \"BUE\",                                                            # city_pair\r\n",
					"            \"AR\",                                                             # country\r\n",
					"            \"CAB_ECONOMY\",                                                    # cabin_class\r\n",
					"            \"AEP\",                                                            # airport_code\r\n",
					"            \"publishedFare\",                                                  # fare_type\r\n",
					"            total_person,                                                     # passengers\r\n",
					"            random.randint(0, 1),                                              # infants\r\n",
					"            random.randint(0, 2),                                              # children\r\n",
					"            total_person,                                                     # adults\r\n",
					"            random.choice(currencies),                                        # currency\r\n",
					"            total_flight_cost,                                                # total_cost\r\n",
					"            total_flight_cost_per_passenger,                                  # cost_per_adult\r\n",
					"            random.choice(baggage),                                           # baggage_allowance\r\n",
					"            date.strftime(\"%Y-%m-%d %H:%M:%S\")                                # last_update\r\n",
					"        ))\r\n",
					"    return rows"
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_dataframe(rows_list):\r\n",
					"\r\n",
					"    # Create RDD from all_rows\r\n",
					"    rdd = spark.sparkContext.parallelize(rows_list)\r\n",
					"\r\n",
					"    # Create DataFrame schema\r\n",
					"    schema = StructType([\r\n",
					"        StructField(\"id\", StringType(), True),\r\n",
					"        StructField(\"flight_code\", StringType(), True),\r\n",
					"        StructField(\"file_path\", StringType(), True),\r\n",
					"        StructField(\"source\", StringType(), True),\r\n",
					"        StructField(\"search_date\", StringType(), True),\r\n",
					"        StructField(\"outbound_departure_datetime\", StringType(), True),\r\n",
					"        StructField(\"inbound_departure_datetime\", StringType(), True),\r\n",
					"        StructField(\"route\", StringType(), True),\r\n",
					"        StructField(\"city_pair\", StringType(), True),\r\n",
					"        StructField(\"country\", StringType(), True),\r\n",
					"        StructField(\"cabin_class\", StringType(), True),\r\n",
					"        StructField(\"airport_code\", StringType(), True),\r\n",
					"        StructField(\"fare_type\", StringType(), True),\r\n",
					"        StructField(\"passengers\", IntegerType(), True),\r\n",
					"        StructField(\"infants\", IntegerType(), True),\r\n",
					"        StructField(\"children\", IntegerType(), True),\r\n",
					"        StructField(\"adults\", IntegerType(), True),\r\n",
					"        StructField(\"currency\", StringType(), True),\r\n",
					"        StructField(\"total_cost\", DoubleType(), True),\r\n",
					"        StructField(\"cost_per_adult\", DoubleType(), True),\r\n",
					"        StructField(\"baggage_allowance\", StringType(), True),\r\n",
					"        StructField(\"last_update\", StringType(), True)\r\n",
					"    ])\r\n",
					"\r\n",
					"    # Create DataFrame from RDD applying the schema\r\n",
					"    df = spark.createDataFrame(rdd, schema=schema)\r\n",
					"    print(\"###--- Dataframe generation executed! ---###\")\r\n",
					"    return df"
				],
				"execution_count": 40
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"'''\r\n",
					"Compression ratio\r\n",
					"\r\n",
					"\r\n",
					"When compressing large datasets partitioned in a data lake using PySpark, the choice of compression codec can significantly affect both the performance and the size of the saved files. Below are some commonly used compression codecs, with a brief overview of their performance and suitability for large datasets:\r\n",
					"\r\n",
					"    Snappy:\r\n",
					"        Pros: Fast compression and decompression, reasonable compression ratio.\r\n",
					"        Cons: Slightly larger file sizes compared to other codecs like gzip or zstd.\r\n",
					"        Best Use: When speed is a priority over the compression ratio.\r\n",
					"    Gzip:\r\n",
					"        Pros: Good compression ratio.\r\n",
					"        Cons: Slower compression and decompression compared to snappy.\r\n",
					"        Best Use: When the compression ratio is more important than speed.\r\n",
					"    LZO:\r\n",
					"        Pros: Fast compression and decompression, reasonable compression ratio.\r\n",
					"        Cons: Not as widely supported as snappy or gzip.\r\n",
					"        Best Use: Similar to snappy, for scenarios where speed is important.\r\n",
					"    Brotli:\r\n",
					"        Pros: Very good compression ratio.\r\n",
					"        Cons: Slower than gzip, but faster than zstd in many cases.\r\n",
					"        Best Use: When you need a good balance between compression ratio and speed.\r\n",
					"    LZ4:\r\n",
					"        Pros: Extremely fast compression and decompression.\r\n",
					"        Cons: Lower compression ratio compared to gzip or zstd.\r\n",
					"        Best Use: When decompression speed is critical (e.g., real-time applications).\r\n",
					"    Zstandard (zstd):\r\n",
					"        Pros: Excellent compression ratio, good speed.\r\n",
					"        Cons: Slightly slower than snappy but generally faster than gzip.\r\n",
					"        Best Use: When you need a high compression ratio with reasonable speed.\r\n",
					"\r\n",
					"Recommended Choice:\r\n",
					"\r\n",
					"    For general purposes: Snappy is often recommended due to its balance of speed and compression ratio. It's particularly suitable for big data processing where quick read/write operations are crucial.\r\n",
					"    For better compression ratio: Zstandard (zstd) is an excellent choice as it offers a good balance of speed and compression efficiency.\r\n",
					"    For maximum compression ratio: Gzip or Brotli can be considered, but they are slower compared to Snappy and Zstandard.\r\n",
					"    \r\n",
					"'''"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def write_datalake(dataframe):\r\n",
					"    # Writing DLGen2\r\n",
					"    account_name = \"stflightshistorical001\"\r\n",
					"    container_name = \"azfs-flights-historical\"\r\n",
					"    relative_path = \"flights-historical\" \r\n",
					"\r\n",
					"    # Broadcast the dataframe if necessary (example use-case)\r\n",
					"    broadcasted_df = broadcast(dataframe)\r\n",
					"\r\n",
					"    # Write DataFrame to Azure Data Lake Gen2 in Parquet format partitioned by search_date and route\r\n",
					"    dataframe.write \\\r\n",
					"        .partitionBy(\"search_date\", \"route\") \\\r\n",
					"        .option(\"compression\", \"zstd\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .parquet(f\"abfss://{container_name}@{account_name}.dfs.core.windows.net/{relative_path}/\")\r\n",
					"    \r\n",
					"    print(f\"###--- Write datalake executed for search_date! ---###\")"
				],
				"execution_count": 47
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create list of rows for all dates\r\n",
					"all_rows = []\r\n",
					"count = 0\r\n",
					"reach = 180\r\n",
					"while count < reach: \r\n",
					"    for date in date_range:\r\n",
					"        all_rows.extend(generate_rows_for_date(date))\r\n",
					"        print(f\"In date rage {date} the total flights founded are {len(all_rows)}\")\r\n",
					"        count += 1\r\n",
					"        df = create_dataframe(all_rows)\r\n",
					"        write_datalake(df)"
				],
				"execution_count": 48
			}
		]
	}
}