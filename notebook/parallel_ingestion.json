{
	"name": "parallel_ingestion",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "7",
				"spark.autotune.trackingId": "deaec957-8454-48da-b506-1d778a996637"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/98fb3285-f648-4b06-80a4-01c259978e8f/resourceGroups/flights-mlops-proof-of-concept/providers/Microsoft.Synapse/workspaces/synw-flights-bronze-layer/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://synw-flights-bronze-layer.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 16,
				"memory": 112,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from azure.storage.blob import BlobServiceClient\r\n",
					"import pandas as pd\r\n",
					"import numpy as np\r\n",
					"import os\r\n",
					"import uuid\r\n",
					"import random\r\n",
					"import json\r\n",
					"import gzip\r\n",
					"import time\r\n",
					"from datetime import datetime, timedelta\r\n",
					"from concurrent.futures import ThreadPoolExecutor, as_completed\r\n",
					"import argparse\r\n",
					"\r\n",
					"# Configure Azure Blob Storage\r\n",
					"blob_service_client = BlobServiceClient.from_connection_string(\"your_connection_string\")\r\n",
					"container_name = 'dev-data-synapse-stresstest'\r\n",
					"prefix = 'data-generator/historical/'  # Folder structure in Blob Storage\r\n",
					"\r\n",
					"# Providers\r\n",
					"providers = [\r\n",
					"    \"TRAVELFUSION/americanAirlines\", \"AMADEUS/voyzant\", \"AMADEUS/picasso\",\r\n",
					"    \"AMADEUS/aerticket\", \"AMADEUS/amadeus\", \"TRAVELFUSION/lufthansa\", \"TRAVELFUSION/travelfusion\"\r\n",
					"]\r\n",
					"\r\n",
					"# Routes\r\n",
					"routes = [\r\n",
					"    'EWR-CAI', 'FLL-LIS', 'JFK-ATH', 'JFK-CAI', 'LAX-ATH', 'LGA-CAI', 'LGW-IST', 'LGW-NRT', 'LHR-AMM',\r\n",
					"    'LHR-DEL', 'LHR-HND', 'LHR-IST', 'LHR-LIM', 'LHR-NRT', 'LHR-SAW', 'MIA-LIS', 'STN-AMM', 'STN-SAW',\r\n",
					"    'YTZ-ATH', 'YTZ-LIS', 'YYZ-ATH', 'YYZ-JNB', 'YYZ-LIS', 'JFK-JNB', 'YYZ-UIO', 'LAX-JNB', 'LHR-JNB',\r\n",
					"    'LHR-SJO', 'SFO-LIS', 'YUL-LIS', 'LAX-LIM', 'YYZ-LIM', 'LAX-LIS', 'MIA-LIM', 'LAX-IST', 'LAX-UIO'\r\n",
					"]\r\n",
					"\r\n",
					"# Currencies\r\n",
					"currencies = [\r\n",
					"    \"EUR\", \"COP\", \"MXN\", \"USD\", \"CAD\", \"GBP\"\r\n",
					"]\r\n",
					"\r\n",
					"# Baggage allowances\r\n",
					"baggage = [\r\n",
					"    \"0PC\", \"1PC\", \"2PC\", \"3PC\"\r\n",
					"]\r\n",
					"\r\n",
					"# Function to generate dummy data for a specific date and route\r\n",
					"def generate_dummy_data(date, route, rows_per_day):\r\n",
					"    data = []\r\n",
					"    for _ in range(rows_per_day):\r\n",
					"        total_person = random.randint(1, 5)\r\n",
					"        total_flight_cost = random.uniform(21.23, 100000.4)\r\n",
					"        total_flight_cost_per_passenger = total_flight_cost / total_person\r\n",
					"        data.append({\r\n",
					"            \"id\": str(uuid.uuid4()),\r\n",
					"            \"flight_code\": f\"L1{{{route}}}_L2{random.randint(1000, 9999)}{random.choice(baggage)}\",\r\n",
					"            \"file_path\": f\"flightHistory/{date.strftime('%Y-%m-%d')}/{uuid.uuid4().hex}.json.gz\",\r\n",
					"            \"source\": random.choice(providers),\r\n",
					"            \"search_date\": date.strftime(\"%Y-%m-%d\"),\r\n",
					"            \"outbound_departure_datetime\": (date + timedelta(days=10, hours=10, minutes=25)).strftime(\"%Y-%m-%d %H:%M:%S\"),\r\n",
					"            \"inbound_departure_datetime\": (date + timedelta(days=15, hours=10, minutes=25)).strftime(\"%Y-%m-%d %H:%M:%S\"),\r\n",
					"            \"route\": route,\r\n",
					"            \"city_pair\": \"BUE\",\r\n",
					"            \"country\": \"AR\",\r\n",
					"            \"cabin_class\": \"CAB_ECONOMY\",\r\n",
					"            \"airport_code\": \"AEP\",\r\n",
					"            \"fare_type\": \"publishedFare\",\r\n",
					"            \"passengers\": total_person,\r\n",
					"            \"infants\": random.randint(0, 1),\r\n",
					"            \"children\": random.randint(0, 2),\r\n",
					"            \"adults\": total_person,\r\n",
					"            \"currency\": random.choice(currencies),\r\n",
					"            \"total_cost\": total_flight_cost,\r\n",
					"            \"cost_per_adult\": total_flight_cost_per_passenger,\r\n",
					"            \"baggage_allowance\": random.choice(baggage),\r\n",
					"            \"last_update\": date.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
					"        })\r\n",
					"    return data\r\n",
					"\r\n",
					"# Function to upload a JSON file (compressed with gzip) to Azure Blob Storage\r\n",
					"def upload_to_blob(data, container_name, prefix, date, route):\r\n",
					"    file_path = f'/tmp/{date.strftime(\"%Y-%m-%d\")}_{route}.json.gz'\r\n",
					"    with gzip.open(file_path, 'wt', compresslevel=1) as f:\r\n",
					"        for record in data:\r\n",
					"            json.dump(record, f) # unnested from array\r\n",
					"            f.write('\\n')\r\n",
					"    blob_client = blob_service_client.get_blob_client(container=container_name, blob=f'{prefix}search_date={date.strftime(\"%Y-%m-%d\")}/route={route}/{date.strftime(\"%Y-%m-%d\")}_{route}.json.gz')\r\n",
					"    with open(file_path, 'rb') as f:\r\n",
					"        blob_client.upload_blob(f, overwrite=True)\r\n",
					"    os.remove(file_path)\r\n",
					"\r\n",
					"def parse_args():\r\n",
					"    parser = argparse.ArgumentParser(description='Generate and upload flight data to Azure Blob Storage')\r\n",
					"    parser.add_argument('--start_date', type=str, required=True, help='Start date in YYYY-MM-DD format')\r\n",
					"    parser.add_argument('--end_date', type=str, required=True, help='End date in YYYY-MM-DD format')\r\n",
					"    parser.add_argument('--rows_per_day', type=int, required=True, help='Number of rows per day')\r\n",
					"    return parser.parse_args()\r\n",
					"\r\n",
					"args = parse_args()\r\n",
					"start_date = datetime.strptime(args.start_date, '%Y-%m-%d')\r\n",
					"end_date = datetime.strptime(args.end_date, '%Y-%m-%d')\r\n",
					"rows_per_day = args.rows_per_day\r\n",
					"\r\n",
					"# Function to generate and upload data for a given route and date range\r\n",
					"def generate_and_upload_data(route, start_date, end_date, rows_per_day):\r\n",
					"    start_time = time.time()\r\n",
					"    date = start_date\r\n",
					"    while date <= end_date:\r\n",
					"        # Generate data for current date and route\r\n",
					"        data = generate_dummy_data(date, route, rows_per_day)\r\n",
					"        \r\n",
					"        # Upload data to Azure Blob Storage\r\n",
					"        upload_to_blob(data, container_name, prefix, date, route)\r\n",
					"        \r\n",
					"        # Print status message\r\n",
					"        print(f\"Uploaded {len(data)} records for {date.strftime('%Y-%m-%d')} and route {route}\")\r\n",
					"        \r\n",
					"        end_time = time.time()\r\n",
					"        print(f\"Time taken to read data from Azure Blob Storage for search_date {date}: {end_time - start_time} seconds\")\r\n",
					"        # Move to the next date\r\n",
					"        date += timedelta(days=1)\r\n",
					"\r\n",
					"# Number of threads to use for parallel execution\r\n",
					"#num_threads = min(len(routes), os.cpu_count() * 2)  # Adjust as needed\r\n",
					"num_threads = 50\r\n",
					"\r\n",
					"# Execute in parallel for each route\r\n",
					"with ThreadPoolExecutor(max_workers=num_threads) as executor:\r\n",
					"    futures = []\r\n",
					"    for route in routes:\r\n",
					"        futures.append(executor.submit(generate_and_upload_data, route, start_date, end_date, rows_per_day))\r\n",
					"    \r\n",
					"    # Wait for all futures to complete\r\n",
					"    for future in as_completed(futures):\r\n",
					"        future.result()\r\n",
					"\r\n",
					"print('Data generation and upload completed.')"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}